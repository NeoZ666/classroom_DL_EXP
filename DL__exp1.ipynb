{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeoZ666/classroom_DL_EXP/blob/main/DL__exp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnrA_PcSEOy2",
        "outputId": "a77469c7-411e-4f7f-8ccb-5ef779405709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7477336618715607\n",
            "Epoch 1000, Loss: 0.6699428590090367\n",
            "Epoch 2000, Loss: 0.546718077490241\n",
            "Epoch 3000, Loss: 0.2736197746733484\n",
            "Epoch 4000, Loss: 0.14457934142202802\n",
            "Epoch 5000, Loss: 0.10199270624253903\n",
            "Epoch 6000, Loss: 0.08134783857791282\n",
            "Epoch 7000, Loss: 0.06898792065703974\n",
            "Epoch 8000, Loss: 0.060650846000803074\n",
            "Epoch 9000, Loss: 0.05458697138135718\n",
            "Epoch 10000, Loss: 0.04994266245758164\n",
            "Epoch 11000, Loss: 0.04624967160474196\n",
            "Epoch 12000, Loss: 0.04322850399278136\n",
            "Epoch 13000, Loss: 0.04070134496769465\n",
            "Epoch 14000, Loss: 0.038549289492137415\n",
            "Epoch 15000, Loss: 0.036689580901116105\n",
            "Epoch 16000, Loss: 0.0350627003322818\n",
            "Epoch 17000, Loss: 0.03362465266677117\n",
            "Epoch 18000, Loss: 0.03234215369760299\n",
            "Epoch 19000, Loss: 0.0311895156679722\n",
            "Epoch 20000, Loss: 0.030146567827863256\n",
            "Epoch 21000, Loss: 0.029197229935277402\n",
            "Epoch 22000, Loss: 0.028328510230819115\n",
            "Epoch 23000, Loss: 0.02752978672647985\n",
            "Epoch 24000, Loss: 0.02679228205078933\n",
            "Epoch 25000, Loss: 0.02610867330303027\n",
            "Epoch 26000, Loss: 0.025472797847941948\n",
            "Epoch 27000, Loss: 0.024879428441817657\n",
            "Epoch 28000, Loss: 0.02432409922887847\n",
            "Epoch 29000, Loss: 0.02380296958310606\n",
            "Epoch 30000, Loss: 0.02331271646445426\n",
            "Epoch 31000, Loss: 0.02285044851024813\n",
            "Epoch 32000, Loss: 0.022413636872701797\n",
            "Epoch 33000, Loss: 0.022000059087012868\n",
            "Epoch 34000, Loss: 0.02160775317233921\n",
            "Epoch 35000, Loss: 0.021234979837462697\n",
            "Epoch 36000, Loss: 0.020880191156788817\n",
            "Epoch 37000, Loss: 0.02054200445039965\n",
            "Epoch 38000, Loss: 0.020219180378901802\n",
            "Epoch 39000, Loss: 0.019910604474204743\n",
            "Epoch 40000, Loss: 0.019615271488515745\n",
            "Epoch 41000, Loss: 0.019332272068289483\n",
            "Epoch 42000, Loss: 0.01906078135667985\n",
            "Epoch 43000, Loss: 0.018800049203903938\n",
            "Epoch 44000, Loss: 0.01854939172477559\n",
            "Epoch 45000, Loss: 0.01830818399017542\n",
            "Epoch 46000, Loss: 0.018075853677167828\n",
            "Epoch 47000, Loss: 0.01785187553296276\n",
            "Epoch 48000, Loss: 0.01763576653253364\n",
            "Epoch 49000, Loss: 0.017427081629696698\n",
            "Epoch 50000, Loss: 0.017225410017758295\n",
            "Epoch 51000, Loss: 0.017030371829210654\n",
            "Epoch 52000, Loss: 0.016841615214958403\n",
            "Epoch 53000, Loss: 0.01665881375266316\n",
            "Epoch 54000, Loss: 0.01648166414135443\n",
            "Epoch 55000, Loss: 0.01630988414575106\n",
            "Epoch 56000, Loss: 0.016143210759013496\n",
            "Epoch 57000, Loss: 0.01598139855707491\n",
            "Epoch 58000, Loss: 0.015824218221431943\n",
            "Epoch 59000, Loss: 0.015671455210427235\n",
            "Epoch 60000, Loss: 0.015522908561738852\n",
            "Epoch 61000, Loss: 0.015378389811064267\n",
            "Epoch 62000, Loss: 0.01523772201393396\n",
            "Epoch 63000, Loss: 0.015100738859249294\n",
            "Epoch 64000, Loss: 0.014967283864574488\n",
            "Epoch 65000, Loss: 0.01483720964443493\n",
            "Epoch 66000, Loss: 0.014710377243934581\n",
            "Epoch 67000, Loss: 0.01458665553092779\n",
            "Epoch 68000, Loss: 0.014465920640771688\n",
            "Epoch 69000, Loss: 0.014348055468376069\n",
            "Epoch 70000, Loss: 0.014232949202872536\n",
            "Epoch 71000, Loss: 0.01412049690074636\n",
            "Epoch 72000, Loss: 0.014010599093740505\n",
            "Epoch 73000, Loss: 0.013903161428234128\n",
            "Epoch 74000, Loss: 0.01379809433316287\n",
            "Epoch 75000, Loss: 0.01369531271385101\n",
            "Epoch 76000, Loss: 0.013594735669406723\n",
            "Epoch 77000, Loss: 0.013496286231566568\n",
            "Epoch 78000, Loss: 0.013399891123100449\n",
            "Epoch 79000, Loss: 0.01330548053406681\n",
            "Epoch 80000, Loss: 0.013212987914386189\n",
            "Epoch 81000, Loss: 0.013122349781346751\n",
            "Epoch 82000, Loss: 0.013033505540789709\n",
            "Epoch 83000, Loss: 0.012946397320841711\n",
            "Epoch 84000, Loss: 0.012860969817170893\n",
            "Epoch 85000, Loss: 0.012777170148834624\n",
            "Epoch 86000, Loss: 0.012694947723873364\n",
            "Epoch 87000, Loss: 0.012614254113884775\n",
            "Epoch 88000, Loss: 0.012535042936877593\n",
            "Epoch 89000, Loss: 0.012457269747767535\n",
            "Epoch 90000, Loss: 0.012380891935934835\n",
            "Epoch 91000, Loss: 0.012305868629313184\n",
            "Epoch 92000, Loss: 0.012232160604521473\n",
            "Epoch 93000, Loss: 0.012159730202598083\n",
            "Epoch 94000, Loss: 0.012088541249927781\n",
            "Epoch 95000, Loss: 0.012018558983987463\n",
            "Epoch 96000, Loss: 0.01194974998357087\n",
            "Epoch 97000, Loss: 0.0118820821031732\n",
            "Epoch 98000, Loss: 0.011815524411250292\n",
            "Epoch 99000, Loss: 0.01175004713208114\n",
            "Final Predictions:\n",
            "Input: [0 0] - Predicted: 0.0129 - Actual: 0\n",
            "Input: [0 1] - Predicted: 0.9889 - Actual: 1\n",
            "Input: [1 0] - Predicted: 0.9889 - Actual: 1\n",
            "Input: [1 1] - Predicted: 0.0114 - Actual: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# XOR input and output data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "np.random.seed(0)  # For reproducibility\n",
        "\n",
        "# Weights and biases for hidden layer\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.random.randn(hidden_size)\n",
        "\n",
        "# Weights and biases for output layer\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.random.randn(output_size)\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 100000\n",
        "\n",
        "# Training the MLP\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer_input = np.dot(X, W1) + b1\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "    output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "\n",
        "    # Calculate the loss (binary cross-entropy)\n",
        "    loss = -np.mean(y * np.log(predicted_output + 1e-8) + (1 - y) * np.log(1 - predicted_output + 1e-8))\n",
        "\n",
        "    # Backward propagation\n",
        "    output_error = predicted_output - y\n",
        "    output_delta = output_error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    hidden_error = np.dot(output_delta, W2.T)\n",
        "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * np.dot(hidden_layer_output.T, output_delta)\n",
        "    b2 -= learning_rate * np.sum(output_delta, axis=0)\n",
        "\n",
        "    W1 -= learning_rate * np.dot(X.T, hidden_delta)\n",
        "    b1 -= learning_rate * np.sum(hidden_delta, axis=0)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Making predictions\n",
        "hidden_layer_input = np.dot(X, W1) + b1\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "predictions = sigmoid(output_layer_input)\n",
        "\n",
        "print(\"Final Predictions:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Input: {X[i]} - Predicted: {pred[0]:.4f} - Actual: {y[i][0]}\")\n"
      ]
    }
  ]
}