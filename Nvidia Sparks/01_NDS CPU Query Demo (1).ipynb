{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c867e37-9db0-4d65-80de-3feee033aa4f",
   "metadata": {},
   "source": [
    "# Welcome to the Spark RAPIDS NDS Demo Lab\n",
    "This notebook will guide you through understanding how to run Spark CPU [NDS queries](https://github.com/NVIDIA/spark-rapids-benchmarks/tree/dev/nds).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772cf7d-ff58-4673-87fa-978668ae6dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start the Spark history server\n",
    "\n",
    "To begin, launch the Spark history server to allow us to view the Spark UI for applications that are completed.  That will help you gain a better understanding of how Spark plans queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af9abc2-ab08-40f8-be28-eddf457c8aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.history.HistoryServer, logging to /opt/spark/logs/spark--org.apache.spark.deploy.history.HistoryServer-1-2813d72834c0.out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/opt/spark/sbin/start-history-server.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909639d-1a4d-440b-a3eb-61c62b0787e9",
   "metadata": {},
   "source": [
    "## Access the Spark history server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f1e18a-3883-4919-bcfe-da356ab89aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var hostname = window.location.hostname;\n",
       "var href = \"'http://\"+hostname+\":18080'\";\n",
       "element.innerHTML = '<a style=\"color:blue;\" target=\"_blank\" href='+href+'>Open Spark History Server</a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var hostname = window.location.hostname;\n",
    "var href = \"'http://\"+hostname+\":18080'\";\n",
    "element.innerHTML = '<a style=\"color:blue;\" target=\"_blank\" href='+href+'>Open Spark History Server</a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d846082-e060-4cd1-a3a0-1286cd3b7455",
   "metadata": {},
   "source": [
    "## View NDS query\n",
    "You can view the SQL query syntax that will be executed as a Spark application only using CPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92dac80-a48c-4bc8-9993-d72504412e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abb391f-9184-4136-8e24-08621fbe1028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- start query 1 in stream 0 using template query4.tpl\n",
      "with year_total as (\n",
      " select c_customer_id customer_id\n",
      "       ,c_first_name customer_first_name\n",
      "       ,c_last_name customer_last_name\n",
      "       ,c_preferred_cust_flag customer_preferred_cust_flag\n",
      "       ,c_birth_country customer_birth_country\n",
      "       ,c_login customer_login\n",
      "       ,c_email_address customer_email_address\n",
      "       ,d_year dyear\n",
      "       ,sum(((ss_ext_list_price-ss_ext_wholesale_cost-ss_ext_discount_amt)+ss_ext_sales_price)/2) year_total\n",
      "       ,'s' sale_type\n",
      " from customer\n",
      "     ,store_sales\n",
      "     ,date_dim\n",
      " where c_customer_sk = ss_customer_sk\n",
      "   and ss_sold_date_sk = d_date_sk\n",
      " group by c_customer_id\n",
      "         ,c_first_name\n",
      "         ,c_last_name\n",
      "         ,c_preferred_cust_flag\n",
      "         ,c_birth_country\n",
      "         ,c_login\n",
      "         ,c_email_address\n",
      "         ,d_year\n",
      " union all\n",
      " select c_customer_id customer_id\n",
      "       ,c_first_name customer_first_name\n",
      "       ,c_last_name customer_last_name\n",
      "       ,c_preferred_cust_flag customer_preferred_cust_flag\n",
      "       ,c_birth_country customer_birth_country\n",
      "       ,c_login customer_login\n",
      "       ,c_email_address customer_email_address\n",
      "       ,d_year dyear\n",
      "       ,sum((((cs_ext_list_price-cs_ext_wholesale_cost-cs_ext_discount_amt)+cs_ext_sales_price)/2) ) year_total\n",
      "       ,'c' sale_type\n",
      " from customer\n",
      "     ,catalog_sales\n",
      "     ,date_dim\n",
      " where c_customer_sk = cs_bill_customer_sk\n",
      "   and cs_sold_date_sk = d_date_sk\n",
      " group by c_customer_id\n",
      "         ,c_first_name\n",
      "         ,c_last_name\n",
      "         ,c_preferred_cust_flag\n",
      "         ,c_birth_country\n",
      "         ,c_login\n",
      "         ,c_email_address\n",
      "         ,d_year\n",
      "union all\n",
      " select c_customer_id customer_id\n",
      "       ,c_first_name customer_first_name\n",
      "       ,c_last_name customer_last_name\n",
      "       ,c_preferred_cust_flag customer_preferred_cust_flag\n",
      "       ,c_birth_country customer_birth_country\n",
      "       ,c_login customer_login\n",
      "       ,c_email_address customer_email_address\n",
      "       ,d_year dyear\n",
      "       ,sum((((ws_ext_list_price-ws_ext_wholesale_cost-ws_ext_discount_amt)+ws_ext_sales_price)/2) ) year_total\n",
      "       ,'w' sale_type\n",
      " from customer\n",
      "     ,web_sales\n",
      "     ,date_dim\n",
      " where c_customer_sk = ws_bill_customer_sk\n",
      "   and ws_sold_date_sk = d_date_sk\n",
      " group by c_customer_id\n",
      "         ,c_first_name\n",
      "         ,c_last_name\n",
      "         ,c_preferred_cust_flag\n",
      "         ,c_birth_country\n",
      "         ,c_login\n",
      "         ,c_email_address\n",
      "         ,d_year\n",
      "         )\n",
      "  select  \n",
      "                  t_s_secyear.customer_id\n",
      "                 ,t_s_secyear.customer_first_name\n",
      "                 ,t_s_secyear.customer_last_name\n",
      "                 ,t_s_secyear.customer_email_address\n",
      " from year_total t_s_firstyear\n",
      "     ,year_total t_s_secyear\n",
      "     ,year_total t_c_firstyear\n",
      "     ,year_total t_c_secyear\n",
      "     ,year_total t_w_firstyear\n",
      "     ,year_total t_w_secyear\n",
      " where t_s_secyear.customer_id = t_s_firstyear.customer_id\n",
      "   and t_s_firstyear.customer_id = t_c_secyear.customer_id\n",
      "   and t_s_firstyear.customer_id = t_c_firstyear.customer_id\n",
      "   and t_s_firstyear.customer_id = t_w_firstyear.customer_id\n",
      "   and t_s_firstyear.customer_id = t_w_secyear.customer_id\n",
      "   and t_s_firstyear.sale_type = 's'\n",
      "   and t_c_firstyear.sale_type = 'c'\n",
      "   and t_w_firstyear.sale_type = 'w'\n",
      "   and t_s_secyear.sale_type = 's'\n",
      "   and t_c_secyear.sale_type = 'c'\n",
      "   and t_w_secyear.sale_type = 'w'\n",
      "   and t_s_firstyear.dyear =  2001\n",
      "   and t_s_secyear.dyear = 2001+1\n",
      "   and t_c_firstyear.dyear =  2001\n",
      "   and t_c_secyear.dyear =  2001+1\n",
      "   and t_w_firstyear.dyear = 2001\n",
      "   and t_w_secyear.dyear = 2001+1\n",
      "   and t_s_firstyear.year_total > 0\n",
      "   and t_c_firstyear.year_total > 0\n",
      "   and t_w_firstyear.year_total > 0\n",
      "   and case when t_c_firstyear.year_total > 0 then t_c_secyear.year_total / t_c_firstyear.year_total else null end\n",
      "           > case when t_s_firstyear.year_total > 0 then t_s_secyear.year_total / t_s_firstyear.year_total else null end\n",
      "   and case when t_c_firstyear.year_total > 0 then t_c_secyear.year_total / t_c_firstyear.year_total else null end\n",
      "           > case when t_w_firstyear.year_total > 0 then t_w_secyear.year_total / t_w_firstyear.year_total else null end\n",
      " order by t_s_secyear.customer_id\n",
      "         ,t_s_secyear.customer_first_name\n",
      "         ,t_s_secyear.customer_last_name\n",
      "         ,t_s_secyear.customer_email_address\n",
      " LIMIT 100;\n",
      "\n",
      "-- end query 1 in stream 0 using template query4.tpl\n"
     ]
    }
   ],
   "source": [
    "%%script env query=\"$query\" bash\n",
    "cat ./query_files/query$query.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166d028-170a-411f-bea1-a844329dad64",
   "metadata": {},
   "source": [
    "## Run NDS query (CPU)\n",
    "The SQL query will be executed as a Spark job on your instance.  It will read data from S3 at a scale factor of 100GB.  Once the query completes, it will output the entire duration for the run, including the table setup time as well as the individual query execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e38ba9-598c-4d66-9259-fb9644acff4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ source power_run_cpu.template\n",
      "++ source base.template\n",
      "+++ export SPARK_HOME=/opt/spark\n",
      "+++ SPARK_HOME=/opt/spark\n",
      "+++ export SPARK_MASTER=yarn\n",
      "+++ SPARK_MASTER=yarn\n",
      "+++ export DRIVER_MEMORY=10G\n",
      "+++ DRIVER_MEMORY=10G\n",
      "+++ export EXECUTOR_CORES=12\n",
      "+++ EXECUTOR_CORES=12\n",
      "+++ export NUM_EXECUTORS=8\n",
      "+++ NUM_EXECUTORS=8\n",
      "+++ export EXECUTOR_MEMORY=16G\n",
      "+++ EXECUTOR_MEMORY=16G\n",
      "+++ export NDS_LISTENER_JAR=./jvm_listener/target/nds-benchmark-listener-1.0-SNAPSHOT.jar\n",
      "+++ NDS_LISTENER_JAR=./jvm_listener/target/nds-benchmark-listener-1.0-SNAPSHOT.jar\n",
      "+++ export SPARK_RAPIDS_PLUGIN_JAR=rapids-4-spark_2.12-22.06.0.jar\n",
      "+++ SPARK_RAPIDS_PLUGIN_JAR=rapids-4-spark_2.12-22.06.0.jar\n",
      "++++ echo /opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "+++ export PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "+++ PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.5-src.zip\n",
      "++ export SHUFFLE_PARTITIONS=200\n",
      "++ SHUFFLE_PARTITIONS=200\n",
      "++ SPARK_CONF=(\"--conf\" \"spark.driver.memory=${DRIVER_MEMORY}\" \"--conf\" \"spark.executor.cores=${EXECUTOR_CORES}\" \"--conf\" \"spark.executor.instances=${NUM_EXECUTORS}\" \"--conf\" \"spark.executor.memory=${EXECUTOR_MEMORY}\" \"--conf\" \"spark.sql.shuffle.partitions=${SHUFFLE_PARTITIONS}\" \"--conf\" \"spark.scheduler.minRegisteredResourcesRatio=1.0\" \"--conf\" \"spark.sql.adaptive.enabled=true\" \"--conf\" \"spark.sql.broadcastTimeout=1200\" \"--conf\" \"spark.dynamicAllocation.enabled=false\" \"--conf\" \"spark.eventLog.enabled=true\" \"--conf\" \"spark.eventLog.dir=/tmp/spark-events\" \"--conf\" \"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\" \"--conf\" \"spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\" \"--conf\" \"spark.jars.packages=org.apache.hadoop:hadoop-aws:3.2.2,com.amazonaws:aws-java-sdk-bundle:1.12.180\" \"--jars\" \"$NDS_LISTENER_JAR\")\n",
      "++ export SPARK_CONF\n",
      "+ MORE_ARGS=(\"${@:2}\")\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\")\n",
      "+ CMD+=(\"${SPARK_CONF[@]}\")\n",
      "+ CMD+=(\"${MORE_ARGS[@]}\")\n",
      "+ /opt/spark/bin/spark-submit --conf spark.driver.memory=10G --conf spark.executor.cores=12 --conf spark.executor.instances=8 --conf spark.executor.memory=16G --conf spark.sql.shuffle.partitions=200 --conf spark.scheduler.minRegisteredResourcesRatio=1.0 --conf spark.sql.adaptive.enabled=true --conf spark.sql.broadcastTimeout=1200 --conf spark.dynamicAllocation.enabled=false --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=/tmp/spark-events --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider --conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.2.2,com.amazonaws:aws-java-sdk-bundle:1.12.180 --jars ./jvm_listener/target/nds-benchmark-listener-1.0-SNAPSHOT.jar nds_power.py s3a://dli-public-datasets-us-west-2/production/x-ds-04-v1/parquet_sf100 ../../query_files/query4.sql cpu-time-4.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9b59ea31-20f8-4678-8f92-b1ecde6e273f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.180 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/hadoop-aws-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.2!hadoop-aws.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.180/aws-java-sdk-bundle-1.12.180.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.180!aws-java-sdk-bundle.jar (1621ms)\n",
      ":: resolution report :: resolve 1334ms :: artifacts dl 1643ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.180 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 by [com.amazonaws#aws-java-sdk-bundle;1.12.180] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   2   |   2   |   1   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9b59ea31-20f8-4678-8f92-b1ecde6e273f\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (262798kB/159ms)\n",
      "24/09/20 05:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/20 05:40:30 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Creating TempView for table customer_address ======\n",
      "Time taken: 5321 millis for table customer_address\n",
      "====== Creating TempView for table customer_demographics ======\n",
      "Time taken: 1106 millis for table customer_demographics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/20 05:40:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Creating TempView for table date_dim ======\n",
      "Time taken: 1052 millis for table date_dim\n",
      "====== Creating TempView for table warehouse ======\n",
      "Time taken: 1000 millis for table warehouse\n",
      "====== Creating TempView for table ship_mode ======\n",
      "Time taken: 987 millis for table ship_mode\n",
      "====== Creating TempView for table time_dim ======\n",
      "Time taken: 1062 millis for table time_dim\n",
      "====== Creating TempView for table reason ======\n",
      "Time taken: 997 millis for table reason\n",
      "====== Creating TempView for table income_band ======\n",
      "Time taken: 1005 millis for table income_band\n",
      "====== Creating TempView for table item ======\n",
      "Time taken: 1274 millis for table item\n",
      "====== Creating TempView for table store ======\n",
      "Time taken: 1010 millis for table store\n",
      "====== Creating TempView for table call_center ======\n",
      "Time taken: 987 millis for table call_center\n",
      "====== Creating TempView for table customer ======\n",
      "Time taken: 1115 millis for table customer\n",
      "====== Creating TempView for table web_site ======\n",
      "Time taken: 997 millis for table web_site\n",
      "====== Creating TempView for table inventory ======\n",
      "Time taken: 5199 millis for table inventory\n",
      "====== Creating TempView for table catalog_returns ======\n",
      "Time taken: 32239 millis for table catalog_returns\n",
      "====== Creating TempView for table web_returns ======\n",
      "Time taken: 33582 millis for table web_returns\n",
      "====== Creating TempView for table web_sales ======\n",
      "Time taken: 28333 millis for table web_sales\n",
      "====== Creating TempView for table catalog_sales ======\n",
      "Time taken: 29274 millis for table catalog_sales\n",
      "====== Creating TempView for table store_sales ======\n",
      "Time taken: 28399 millis for table store_sales\n",
      "====== Run query4 ======\n",
      "TaskFailureListener is registered.\n",
      "Time taken: [347214] millis for query4\n",
      "====== Power Test Time: 348000 milliseconds ======\n",
      "====== Total Time: 565107 milliseconds ======\n",
      "['application_id', 'query', 'time/milliseconds']\n",
      "('local-1726810823666', 'CreateTempView customer_address', 5321)\n",
      "('local-1726810823666', 'CreateTempView customer_demographics', 1106)\n",
      "('local-1726810823666', 'CreateTempView date_dim', 1052)\n",
      "('local-1726810823666', 'CreateTempView warehouse', 1000)\n",
      "('local-1726810823666', 'CreateTempView ship_mode', 987)\n",
      "('local-1726810823666', 'CreateTempView time_dim', 1062)\n",
      "('local-1726810823666', 'CreateTempView reason', 997)\n",
      "('local-1726810823666', 'CreateTempView income_band', 1005)\n",
      "('local-1726810823666', 'CreateTempView item', 1274)\n",
      "('local-1726810823666', 'CreateTempView store', 1010)\n",
      "('local-1726810823666', 'CreateTempView call_center', 987)\n",
      "('local-1726810823666', 'CreateTempView customer', 1115)\n",
      "('local-1726810823666', 'CreateTempView web_site', 997)\n",
      "('local-1726810823666', 'CreateTempView store_returns', 31121)\n",
      "('local-1726810823666', 'CreateTempView household_demographics', 956)\n",
      "('local-1726810823666', 'CreateTempView web_page', 956)\n",
      "('local-1726810823666', 'CreateTempView promotion', 986)\n",
      "('local-1726810823666', 'CreateTempView catalog_page', 999)\n",
      "('local-1726810823666', 'CreateTempView inventory', 5199)\n",
      "('local-1726810823666', 'CreateTempView catalog_returns', 32239)\n",
      "('local-1726810823666', 'CreateTempView web_returns', 33582)\n",
      "('local-1726810823666', 'CreateTempView web_sales', 28333)\n",
      "('local-1726810823666', 'CreateTempView catalog_sales', 29274)\n",
      "('local-1726810823666', 'CreateTempView store_sales', 28399)\n",
      "('local-1726810823666', 'query4', 347214)\n",
      "('local-1726810823666', 'Power Start Time', 1726811039)\n",
      "('local-1726810823666', 'Power End Time', 1726811387)\n",
      "('local-1726810823666', 'Power Test Time', 348000)\n",
      "('local-1726810823666', 'Total Time', 565107)\n"
     ]
    }
   ],
   "source": [
    "%%script env query=\"$query\" bash\n",
    "cd ./spark-rapids-benchmarks/nds\n",
    "./spark-submit-template power_run_cpu.template \\\n",
    "    nds_power.py \\\n",
    "    s3a://dli-public-datasets-us-west-2/production/x-ds-04-v1/parquet_sf100 \\\n",
    "    ../../query_files/query$query.sql \\\n",
    "    cpu-time-$query.csv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
